# -recursive-language-model-48m-
Custom 48M parameter transformer-based language model built from scratch with adaptive recursion depth routing (inspired by Mixture-of-Depths). Trained on 100k diverse English documents using PyTorch &amp; GPT-2 tokenizer, achieving 46.75 validation perplexity. Full code, training notebook &amp; details inside.
